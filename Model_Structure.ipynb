{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "parameter_Order = [0,1,2,0,1,3,0,2,1,0,2,3,0,3,1,0,3,2,1,3,0,1]\n",
    "parameterN      = len(parameter_Order)\n",
    "target_label    ='fix_10days_tp5_sl2_labels_B'\n",
    "trainDays       = [\"2000-1-1\",\"2016-1-1\"]\n",
    "valDays         = [\"2016-1-1\",\"2018-1-1\"]\n",
    "testDays        = [\"2018-1-1\",\"2020-1-1\"]\n",
    "\n",
    "#model\n",
    "savepath    = \"Stockmodel\"\n",
    "subsavepath = \"model_5\"\n",
    "epochs = 100\n",
    "batchs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version= 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow version=\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" #only error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import slim as contrib_slim\n",
    "\n",
    "slim = contrib_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_nearest(net, num, kernel_size, stride=1, scope=\"\", activation_fn=tf.nn.relu, normalizer_fn=None):\n",
    "    pad_total = kernel_size - 1 if type(kernel_size)!=list else kernel_size[0] - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    inputs = tf.pad(net, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], mode=\"SYMMETRIC\")\n",
    "    return slim.conv2d(inputs, num, kernel_size, stride=stride, padding='VALID', scope=scope)\n",
    "\n",
    "def avg_pool2d_nearest(net, kernel_size, stride=1, scope=\"\"):\n",
    "    pad_total = kernel_size - 1 if type(kernel_size)!=list else kernel_size[0] - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    inputs = tf.pad(net, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], mode=\"SYMMETRIC\")\n",
    "    return slim.avg_pool2d(inputs, kernel_size, stride=stride, padding='VALID', scope=scope)\n",
    "\n",
    "def block319_nearest(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None, kernel_size=[3,3], ratio=1):\n",
    "    \n",
    "    with tf.compat.v1.variable_scope(scope, 'Block319', [net], reuse=reuse):\n",
    "        with tf.compat.v1.variable_scope('Branch_0'):\n",
    "            tower_conv    = conv2d_nearest(net          , 32*ratio, 1, scope='Conv2d_1x1')\n",
    "        with tf.compat.v1.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = conv2d_nearest(net          , 32*ratio, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = conv2d_nearest(tower_conv1_0, 32*ratio, kernel_size, scope='Conv2d_0b_{0}x{1}'.format(*kernel_size))\n",
    "        with tf.compat.v1.variable_scope('Branch_2'):\n",
    "            tower_conv2_0 = conv2d_nearest(net          , 32*ratio, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv2_1 = conv2d_nearest(tower_conv2_0, 48*ratio, kernel_size, scope='Conv2d_0b_{0}x{1}'.format(*kernel_size))\n",
    "            tower_conv2_2 = conv2d_nearest(tower_conv2_1, 64*ratio, kernel_size, scope='Conv2d_0c_{0}x{1}'.format(*kernel_size))\n",
    "        with tf.compat.v1.variable_scope('Branch_3'):\n",
    "            tower_conv3_0 = conv2d_nearest(net          , 32*ratio, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv3_1 = conv2d_nearest(tower_conv3_0, 48*ratio, kernel_size, scope='Conv2d_0b_{0}x{1}'.format(*kernel_size))\n",
    "            tower_conv3_2 = conv2d_nearest(tower_conv3_1, 64*ratio, kernel_size, scope='Conv2d_0c_{0}x{1}'.format(*kernel_size))\n",
    "            tower_conv3_3 = conv2d_nearest(tower_conv3_2, 64*ratio, kernel_size, scope='Conv2d_0d_{0}x{1}'.format(*kernel_size))\n",
    "        with tf.compat.v1.variable_scope('Branch_4'):\n",
    "            tower_conv4_0 = conv2d_nearest(net          , 32*ratio, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv4_1 = conv2d_nearest(tower_conv4_0, 48*ratio, kernel_size, scope='Conv2d_0b_{0}x{1}'.format(*kernel_size))\n",
    "            tower_conv4_2 = conv2d_nearest(tower_conv4_1, 48*ratio, kernel_size, scope='Conv2d_0c_{0}x{1}'.format(*kernel_size))\n",
    "            tower_conv4_3 = conv2d_nearest(tower_conv4_2, 64*ratio, kernel_size, scope='Conv2d_0d_{0}x{1}'.format(*kernel_size))\n",
    "            tower_conv4_4 = conv2d_nearest(tower_conv4_3, 64*ratio, kernel_size, scope='Conv2d_0e_{0}x{1}'.format(*kernel_size))\n",
    "        mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_3, tower_conv4_4])\n",
    "        up = conv2d_nearest(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope='Conv2d_1x1')\n",
    "        scaled_up = up * scale\n",
    "        if activation_fn == tf.nn.relu6:\n",
    "            # Use clip_by_value to simulate bandpass activation.\n",
    "            scaled_up = tf.clip_by_value(scaled_up, -6.0, 6.0)\n",
    "\n",
    "        net += scaled_up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block319(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None, kerenlsize=[3,1]):\n",
    "    \n",
    "    with tf.compat.v1.variable_scope(scope, 'Block319', [net], reuse=reuse):\n",
    "        with tf.compat.v1.variable_scope('Branch_0'):\n",
    "            tower_conv    = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n",
    "        with tf.compat.v1.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, kerenlsize, scope='Conv2d_0b_{0}x{1}'.format(*kerenlsize))\n",
    "        with tf.compat.v1.variable_scope('Branch_2'):\n",
    "            tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, kerenlsize, scope='Conv2d_0b_{0}x{1}'.format(*kerenlsize))\n",
    "            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, kerenlsize, scope='Conv2d_0c_{0}x{1}'.format(*kerenlsize))\n",
    "        with tf.compat.v1.variable_scope('Branch_3'):\n",
    "            tower_conv3_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv3_1 = slim.conv2d(tower_conv3_0, 48, kerenlsize, scope='Conv2d_0b_{0}x{1}'.format(*kerenlsize))\n",
    "            tower_conv3_2 = slim.conv2d(tower_conv3_1, 64, kerenlsize, scope='Conv2d_0c_{0}x{1}'.format(*kerenlsize))\n",
    "            tower_conv3_3 = slim.conv2d(tower_conv3_2, 64, kerenlsize, scope='Conv2d_0d_{0}x{1}'.format(*kerenlsize))\n",
    "        with tf.compat.v1.variable_scope('Branch_4'):\n",
    "            tower_conv4_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv4_1 = slim.conv2d(tower_conv4_0, 48, kerenlsize, scope='Conv2d_0b_{0}x{1}'.format(*kerenlsize))\n",
    "            tower_conv4_2 = slim.conv2d(tower_conv4_1, 48, kerenlsize, scope='Conv2d_0c_{0}x{1}'.format(*kerenlsize))\n",
    "            tower_conv4_3 = slim.conv2d(tower_conv4_2, 64, kerenlsize, scope='Conv2d_0d_{0}x{1}'.format(*kerenlsize))\n",
    "            tower_conv4_4 = slim.conv2d(tower_conv4_3, 64, kerenlsize, scope='Conv2d_0e_{0}x{1}'.format(*kerenlsize))\n",
    "        mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_3, tower_conv4_4])\n",
    "        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope='Conv2d_1x1')\n",
    "        scaled_up = up * scale\n",
    "        if activation_fn == tf.nn.relu6:\n",
    "            # Use clip_by_value to simulate bandpass activation.\n",
    "            scaled_up = tf.clip_by_value(scaled_up, -6.0, 6.0)\n",
    "\n",
    "        net += scaled_up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax([[1,2],[3,2]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_structure_1(inputs, output=2024, activation_fn=tf.nn.relu, scope=None):\n",
    "    end_points = {}\n",
    "    \n",
    "    with tf.compat.v1.variable_scope(scope, 'InceptionResnet_Stock', [inputs]):\n",
    "        with slim.arg_scope([slim.conv2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "  \n",
    "            net = conv2d_nearest(inputs, 32, 3, stride=(1,1), scope='Conv2d_1a_3x3')\n",
    "            net = conv2d_nearest(net   , 32, 3, stride=(2,1), scope='Conv2d_2a_3x3')\n",
    "            net = conv2d_nearest(net   , 32, 3, stride=(1,1), scope='Conv2d_3a_3x3')\n",
    "\n",
    "            net = conv2d_nearest(net   , 64, 3, stride=(1,1), scope='Conv2d_1b_3x3')\n",
    "            net = conv2d_nearest(net   , 64, 3, stride=(2,1), scope='Conv2d_2b_3x3')\n",
    "            net = conv2d_nearest(net   , 64, 3, stride=(1,1), scope='Conv2d_3b_3x3')\n",
    "            net = conv2d_nearest(net   , 80, 1, stride=(1,1), scope='Conv2d_4b_1x1')\n",
    "\n",
    "            net = conv2d_nearest(net   ,192, 3, stride=(1,1), scope='Conv2d_1c_3x3')\n",
    "            net = conv2d_nearest(net   ,192, 3, stride=(2,1), scope='Conv2d_2c_3x3')\n",
    "            net = conv2d_nearest(net   ,192, 3, stride=(1,1), scope='Conv2d_3c_3x3')\n",
    "\n",
    "            with tf.compat.v1.variable_scope('Mixed_1'):\n",
    "                with tf.compat.v1.variable_scope('Branch_0'):\n",
    "                    tower_conv    = conv2d_nearest(net          , 96, 1, scope='Conv2d_1x1')\n",
    "                with tf.compat.v1.variable_scope('Branch_1'):\n",
    "                    tower_conv1_0 = conv2d_nearest(net          , 48, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv1_1 = conv2d_nearest(tower_conv1_0, 64, 5, scope='Conv2d_0b_5x5')\n",
    "                with tf.compat.v1.variable_scope('Branch_2'):\n",
    "                    tower_conv2_0 = conv2d_nearest(net          , 64, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv2_1 = conv2d_nearest(tower_conv2_0, 96, 3, scope='Conv2d_0b_3x3')\n",
    "                    tower_conv2_2 = conv2d_nearest(tower_conv2_1, 96, 3, scope='Conv2d_0c_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_3'):\n",
    "                    tower_conv3_0 = conv2d_nearest(net          , 64, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv3_1 = conv2d_nearest(tower_conv3_0, 64, 3, scope='Conv2d_0b_3x3')\n",
    "                    tower_conv3_2 = conv2d_nearest(tower_conv3_1, 96, 3, scope='Conv2d_0c_3x3')\n",
    "                    tower_conv3_3 = conv2d_nearest(tower_conv3_2, 96, 3, scope='Conv2d_0d_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_4'):\n",
    "                    tower_pool    = avg_pool2d_nearest(net, 3, stride=1, scope='AvgPool_0a_3x3')\n",
    "                    tower_pool_1  = conv2d_nearest(tower_pool   , 64, 1, scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_3, tower_pool_1], 3)\n",
    "\n",
    "            net = slim.repeat(net, 10, block319_nearest, scale=0.17, activation_fn=activation_fn, ratio=1)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('Mixed_2'):\n",
    "                with tf.compat.v1.variable_scope('Branch_0'):\n",
    "                    tower_conv    = conv2d_nearest(net          , 384, 3, scope='Conv2d_1a_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_1'):\n",
    "                    tower_conv1_0 = conv2d_nearest(net          , 196, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv1_1 = conv2d_nearest(tower_conv1_0, 256, 3, scope='Conv2d_0b_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_2'):\n",
    "                    tower_conv2_0 = conv2d_nearest(net          , 256, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv2_1 = conv2d_nearest(tower_conv2_0, 256, 3, scope='Conv2d_0b_3x3')\n",
    "                    tower_conv2_2 = conv2d_nearest(tower_conv2_1, 384, 3, scope='Conv2d_0c_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_3'):\n",
    "                    tower_conv3_0 = conv2d_nearest(net          , 256, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv3_1 = conv2d_nearest(tower_conv3_0, 256, 3, scope='Conv2d_0b_3x3')\n",
    "                    tower_conv3_2 = conv2d_nearest(tower_conv3_1, 384, 3, scope='Conv2d_0c_3x3')\n",
    "                    tower_conv3_3 = conv2d_nearest(tower_conv3_2, 384, 3, scope='Conv2d_0d_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_4'):\n",
    "                    tower_pool    = avg_pool2d_nearest(net, 3,  stride=1, scope='AvgPool_0a_3x3')\n",
    "                    tower_pool_1  = conv2d_nearest(tower_pool   , 256, 1, scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_3, tower_pool_1], 3)\n",
    "\n",
    "            net = slim.repeat(net, 10, block319_nearest, scale=0.17, activation_fn=activation_fn, ratio=2)\n",
    "\n",
    "            with tf.compat.v1.variable_scope('Mixed_3'):\n",
    "                with tf.compat.v1.variable_scope('Branch_0'):\n",
    "                    tower_conv    = conv2d_nearest(net          , 384, 3, scope='Conv2d_1a_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_1'):\n",
    "                    tower_conv1_0 = conv2d_nearest(net          , 196, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv1_1 = conv2d_nearest(tower_conv1_0, 256, 3, scope='Conv2d_0b_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_2'):\n",
    "                    tower_conv2_0 = conv2d_nearest(net          , 256, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv2_1 = conv2d_nearest(tower_conv2_0, 256, 3, scope='Conv2d_0b_3x3')\n",
    "                    tower_conv2_2 = conv2d_nearest(tower_conv2_1, 384, 3, scope='Conv2d_0c_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_3'):\n",
    "                    tower_conv3_0 = conv2d_nearest(net          , 256, 1, scope='Conv2d_0a_1x1')\n",
    "                    tower_conv3_1 = conv2d_nearest(tower_conv3_0, 256, 3, scope='Conv2d_0b_3x3')\n",
    "                    tower_conv3_2 = conv2d_nearest(tower_conv3_1, 384, 3, scope='Conv2d_0c_3x3')\n",
    "                    tower_conv3_3 = conv2d_nearest(tower_conv3_2, 384, 3, scope='Conv2d_0d_3x3')\n",
    "                with tf.compat.v1.variable_scope('Branch_4'):\n",
    "                    tower_pool    = avg_pool2d_nearest(net, 3,  stride=1, scope='AvgPool_0a_3x3')\n",
    "                net = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_3, tower_pool], 3)\n",
    "\n",
    "            net = slim.repeat(net, 10, block319_nearest, scale=0.17, activation_fn=activation_fn, ratio=3)\n",
    "            net = conv2d_nearest(net, output, 1, scope='Conv2d_base_structure_1_1x1')\n",
    "            end_points['Conv2d_base_structure_1_1x1']=net\n",
    "        return net, end_points\n",
    "\n",
    "def base_structure_2(inputs, num_classes, is_training=True, dropout_keep_prob=0.8, reuse=None,activation_fn=tf.nn.relu, scope=\"InceptionResnet_Stock\"):\n",
    "    with tf.compat.v1.variable_scope(scope, 'InceptionResnet_Stock', [inputs], reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "            \n",
    "            net, end_points = base_structure_1(inputs, scope=scope, activation_fn=activation_fn)\n",
    "            \n",
    "            with tf.compat.v1.variable_scope('Logits'):\n",
    "\n",
    "                kernel_size = net.get_shape()[1:3]\n",
    "                if kernel_size.is_fully_defined():\n",
    "                    net = slim.avg_pool2d(net, kernel_size, padding='VALID', scope='AvgPool_1a_txt')\n",
    "                else:\n",
    "                    net = tf.reduce_mean(input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')\n",
    "                end_points['global_pool'] = net\n",
    "\n",
    "                net = slim.flatten(net)\n",
    "                net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='Dropout')\n",
    "                end_points['PreLogitsFlatten'] = net\n",
    "                \n",
    "                logits = slim.fully_connected(net, num_classes, activation_fn=None, scope='Logits')\n",
    "                end_points['Logits'] = logits\n",
    "                end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n",
    "    return logits, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #--------fix_10days_tp5_sl2_labels_B\n",
      " 0.0    2810\n",
      "-1.0    2045\n",
      " 1.0     168\n",
      "Name: fix_10days_tp5_sl2_labels_B, dtype: int64\n",
      "\n",
      " #--------fix_10days_tp5_sl2_labels_S\n",
      " 0.0    2654\n",
      "-1.0    2137\n",
      " 1.0     232\n",
      "Name: fix_10days_tp5_sl2_labels_S, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import fixDay_preprocess_2 as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess.grabData(label=target_label, startT=trainDays[0], endT=trainDays[1])\n",
    "train_x, train_y = train_x[66:], train_y[66:]\n",
    "val_x  , val_y   = preprocess.grabData(label=target_label, startT=valDays[0]  , endT=valDays[1])\n",
    "test_x , test_y  = preprocess.grabData(label=target_label, startT=testDays[0] , endT=testDays[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1328"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(val_y==[1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Train Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "main_graph = tf.Graph()\n",
    "sess = tf.Session(graph=main_graph)\n",
    "\n",
    "with main_graph.as_default():\n",
    "    node     = tf.placeholder(shape=[1, None, parameterN, 1], dtype=tf.float32)\n",
    "    ans      = tf.placeholder(dtype=tf.float32, shape=(1, 3))\n",
    "    #is_train = tf.placeholder(tf.bool)\n",
    "    is_train = tf.placeholder_with_default(False, shape=(), name=\"is_training\")\n",
    "    \n",
    "    logits, end_points = base_structure_2(node, 3, is_training=is_train, dropout_keep_prob=0.8, activation_fn=tf.nn.tanh) #tf.nn.relu)\n",
    "    loss     = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ans, logits=logits))\n",
    "    \n",
    "    opt      = tf.train.AdamOptimizer(2e-6, beta1=0.5, beta2=0.999)\n",
    "    update   = opt.minimize(loss) \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()    \n",
    "    \n",
    "tf.summary.FileWriter(\"./detect\", graph=tf.get_default_graph())\n",
    "sess.run(init)\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import *\n",
    "\n",
    "model_name = \"Stock_InceptionResnet\"\n",
    "\n",
    "model_dict = {\n",
    "    'model_name' : model_name,\n",
    "    'checkpoint' : Model_checkpoint(os.path.join(savepath, 'model', model_name), save_best_only=True),\n",
    "    'train_batch_log' : History(['loss']),\n",
    "    'val_batch_log' : History(['loss']),\n",
    "    'history' : {\n",
    "        'train_loss':[],\n",
    "        'val_loss':[]\n",
    "    }\n",
    "}\n",
    "\n",
    "callback_dict = {\n",
    "    'on_session_begin':[], # start of a session\n",
    "    'on_batch_begin':[], # start of a training batch\n",
    "    'on_batch_end':[], # end of a training batch\n",
    "    'on_epoch_begin':[], # start of a epoch\n",
    "    'on_epoch_end':[\n",
    "        model_dict['checkpoint']\n",
    "    ], # end of a epoch\n",
    "    'on_session_end':[] # end of a session\n",
    "}\n",
    "callback_manager = Run_collected_functions(callback_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    ### train ###\n",
    "    for batch in range(batchs):\n",
    "        y = np.array([[0,0,0]])\n",
    "        y[0, np.random.randint(3)]=1\n",
    "        \n",
    "        x = np.where(np.all(train_y==y[0],axis=1))[0]\n",
    "        x = np.random.choice(x, 1)[0]\n",
    "        x1, x2 = train_x[x]\n",
    "\n",
    "        # 執行 loss & update (train)\n",
    "        _, loss = sess.run([update, loss], feed_dict={node:x1[np.newaxis, :,parameter_Order, np.newaxis], ans:y, is_train:True})\n",
    "        model_dict['train_batch_log'].push({'loss':loss})\n",
    "    \n",
    "    model_dict['history']['train_loss'].append(model_dict['train_batch_log'].avg_value('loss'))\n",
    "    model_dict['train_batch_log'].reset()\n",
    "\n",
    "    ### val ###\n",
    "    for (x1,x2), y in zip(val_x, val_y):\n",
    "        loss = sess.run(loss, feed_dict={node:x1[np.newaxis, :,parameter_Order, np.newaxis], ans:y[np.newaxis,:], is_train:False})\n",
    "        model_dict['val_batch_log'].push({'loss':loss})\n",
    "\n",
    "    model_dict['history']['val_loss'].append(model_dict['val_batch_log'].avg_value('loss'))\n",
    "    model_dict['val_batch_log'].reset()\n",
    "\n",
    "    ### callback ###\n",
    "    print('Epoch: {}/{}'.format(epoch,epochs))\n",
    "    print('train_loss: {:.3f}'.format(model_dict['history']['train_loss'][-1]))\n",
    "    print('val_loss: {:.3f}'.format(model_dict['history']['val_loss'][-1]))\n",
    "\n",
    "    callback_manager.run_on_epoch_end(val_loss = model_dict['history']['val_loss'][-1],\n",
    "                                      sess = sess,\n",
    "                                      saver = saver,\n",
    "                                      nth_epoch = epoch)\n",
    "    print('############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
